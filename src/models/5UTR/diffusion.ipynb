{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5aa13755-18ba-47b5-8356-750f32ac9502",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ee95ebf2a44fee97f0ae000d81cdc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a0863591-9346-44d7-8ed1-d98d60aa970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass\n",
    "\n",
    "# @dataclass\n",
    "# class TrainingConfig:\n",
    "#     # dataloader\n",
    "#     data_path = \"5utr_95.pt\"\n",
    "#     batch = 64\n",
    "#     train_prop = 0.8\n",
    "#     valid_prop = 0.2\n",
    "#     shuffle = True\n",
    "\n",
    "#     # model\n",
    "#     input_length = 512\n",
    "#     in_channels = 5\n",
    "#     out_channels = 5\n",
    "#     layers_per_block = 5\n",
    "#     block_out_channels = [256, 256, 512, 512]  # 4 blocks each side\n",
    "#     down_block_types = [\"DownBlock1D\", \"DownBlock1D\", \"AttnDownBlock1D\", \"DownBlock1D\"]\n",
    "#     up_block_types = [\"UpBlock1D\", \"AttnUpBlock1D\", \"UpBlock1D\", \"UpBlock1D\"]\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     # train\n",
    "#     scheduler = \"DDPM\"  # ['DDPM', 'DDIM']\n",
    "#     num_train_timesteps = 1000\n",
    "#     optimizer = \"AdamW\"  # ['AdamW', ...]\n",
    "#     lr_warmup_steps = 500\n",
    "#     epoch = 1000\n",
    "#     lr = 1e-5\n",
    "\n",
    "#     # log\n",
    "#     save_model_epochs = 10\n",
    "#     save_path = \"./save_models\"\n",
    "#     # mixed_precision = 'fp16'\n",
    "    \n",
    "#     seed = 2024\n",
    "    \n",
    "# config = TrainingConfig()\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # dataloader\n",
    "    data_path = \"5utr_95_tmp.pt\"\n",
    "    batch = 64\n",
    "    train_prop = 0.8\n",
    "    valid_prop = 0.2\n",
    "    shuffle = True\n",
    "\n",
    "    # model\n",
    "    input_length = 512\n",
    "    in_channels = 5\n",
    "    out_channels = 5\n",
    "    layers_per_block = 5\n",
    "    block_out_channels = [256, 256, 512, 512]  # 4 blocks each side\n",
    "    down_block_types = [\"DownBlock1D\", \"DownBlock1D\", \"AttnDownBlock1D\", \"DownBlock1D\"]\n",
    "    up_block_types = [\"UpBlock1D\", \"AttnUpBlock1D\", \"UpBlock1D\", \"UpBlock1D\"]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # train\n",
    "    scheduler = \"DDPM\"  # ['DDPM', 'DDIM']\n",
    "    num_train_timesteps = 50\n",
    "    optimizer = \"AdamW\"  # ['AdamW', ...]\n",
    "    lr_warmup_steps = 10\n",
    "    epoch = 3\n",
    "    lr = 1e-5\n",
    "\n",
    "    # log\n",
    "    save_model_epochs = 1\n",
    "    save_path = \"./save_models\"\n",
    "    \n",
    "    seed = 2024\n",
    "    \n",
    "config = TrainingConfig()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87f5e1a-75bb-4467-9013-bf09a6552b20",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b7250497-dc45-490c-b318-1be52dce73fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "\n",
    "loaded_sequences = torch.load(config.data_path, weights_only=True)\n",
    "\n",
    "dataset = TensorDataset(loaded_sequences)\n",
    "\n",
    "train_size = int(config.train_prop * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config.batch, shuffle=config.shuffle)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=config.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "543f0f08-fb79-40f9-8602-170443fb8f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch shape: torch.Size([64, 5, 512])\n",
      "Validation batch shape: torch.Size([64, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "for train_batch in train_dataloader:\n",
    "    print(\"Train batch shape:\", train_batch[0].shape)\n",
    "    break\n",
    "    \n",
    "for val_batch in val_dataloader:\n",
    "    print(\"Validation batch shape:\", val_batch[0].shape) \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022d95c7-57c0-4d84-9b53-32b9290cd76a",
   "metadata": {},
   "source": [
    "## Create a UNet1DModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8885361b-fbbc-43bb-95eb-d238233072a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet1DModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class UNet1DWithSoftmax(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet1DWithSoftmax, self).__init__()\n",
    "        self.unet = UNet1DModel(\n",
    "            sample_size = config.input_length,  # the input length of data\n",
    "            in_channels = config.in_channels,  # the one-hot encoded data\n",
    "            out_channels = config.out_channels,  # reconstructed channel of data (also 5, cuz we need gain a sequence)\n",
    "            layers_per_block = config.layers_per_block,  # how many ResNet layers to use per UNet block\n",
    "            block_out_channels = config.block_out_channels,  # block output channels on each side\n",
    "            down_block_types = config.down_block_types,\n",
    "            up_block_types = config.up_block_types\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)  # apply to channels (=>5, 512)\n",
    "\n",
    "    def forward(self, x, timesteps, return_dict=False):\n",
    "        x = self.unet(x, timesteps, return_dict=return_dict)[0]\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = UNet1DWithSoftmax().to(config.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b3b090-49b3-4b8b-93a5-c5aab0606e26",
   "metadata": {},
   "source": [
    "## Create a Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "109d6ebf-4f19-47b0-ac04-61c547b4aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler, DDIMScheduler\n",
    "\n",
    "if config.scheduler == \"DDPM\":\n",
    "    scheduler = DDPMScheduler(num_train_timesteps=config.num_train_timesteps)  # clip_sample: True?\n",
    "elif config.scheduler == \"DDIM\":\n",
    "    scheduler = DDIMScheduler(num_train_timesteps=config.num_train_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "74c4fb88-bc9a-4649-8188-6af53ead08f3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # test the loss\n",
    "# noise = torch.randn(1, 5, 512).to(device)  # (batch_size, in_channels, length)\n",
    "# timesteps = torch.tensor([500]).to(device)\n",
    "# with torch.no_grad():\n",
    "#     noisy_output = model(noise, timesteps).sample\n",
    "\n",
    "# print(noisy_output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebf3066-42f7-4368-9025-0c58c8fb9ed6",
   "metadata": {},
   "source": [
    "## training preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "752b0286-f626-4257-aacd-5b8ffaaa3a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer = optimizer, \n",
    "    num_warmup_steps = config.lr_warmup_steps,\n",
    "    num_training_steps = (len(train_dataloader) * config.epoch),\n",
    ")\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4c880f06-616f-4dc5-8923-87a0e8995e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(config, epoch, pipeline):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6974adb-b18a-4a27-ad15-f03e222980c4",
   "metadata": {},
   "source": [
    "**Wandb setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "540be6ad-7ea9-463c-b66d-8581defa3c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:my37qe8z) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.043 MB of 0.043 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glorious-donkey-1</strong> at: <a href='https://wandb.ai/liwen-wu0821-loveleo/5utr-diffusion-tmp/runs/my37qe8z' target=\"_blank\">https://wandb.ai/liwen-wu0821-loveleo/5utr-diffusion-tmp/runs/my37qe8z</a><br/> View project at: <a href='https://wandb.ai/liwen-wu0821-loveleo/5utr-diffusion-tmp' target=\"_blank\">https://wandb.ai/liwen-wu0821-loveleo/5utr-diffusion-tmp</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240823_042810-my37qe8z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:my37qe8z). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/liwenwu/files/mRNA_diff/src/models/5UTR/wandb/run-20240823_055312-zp8bgl8t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/liwen-wu0821-loveleo/5utr-diffusion-tmp/runs/zp8bgl8t' target=\"_blank\">jumping-firefly-5</a></strong> to <a href='https://wandb.ai/liwen-wu0821-loveleo/5utr-diffusion-tmp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/liwen-wu0821-loveleo/5utr-diffusion-tmp' target=\"_blank\">https://wandb.ai/liwen-wu0821-loveleo/5utr-diffusion-tmp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/liwen-wu0821-loveleo/5utr-diffusion-tmp/runs/zp8bgl8t' target=\"_blank\">https://wandb.ai/liwen-wu0821-loveleo/5utr-diffusion-tmp/runs/zp8bgl8t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/liwen-wu0821-loveleo/5utr-diffusion-tmp/runs/zp8bgl8t?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f9bee5bdc10>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wandb\n",
    "import wandb\n",
    "# initialize wandb\n",
    "wandb.require(\"core\")\n",
    "wandb.login()\n",
    "\n",
    "wandb.init(\n",
    "    project = \"5utr-diffusion-tmp\",\n",
    "    config = config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7578a590-9aca-4b89-9bbe-2e3370f9bdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_metric.Metric at 0x7f9bdbe8e130>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.watch(model, log='all', log_freq=10, log_graph=True)  # log weights of model every 1000 batches\n",
    "wandb.config.system = {\n",
    "    \"monitor\": True,\n",
    "}\n",
    "wandb.define_metric(\"global_step\")  # every batch\n",
    "wandb.define_metric(\"epoch\")\n",
    "wandb.define_metric(\"train_loss/batch\", step_metric=\"global_step\")\n",
    "wandb.define_metric(\"lr/batch\", step_metric=\"global_step\")\n",
    "wandb.define_metric(\"train_loss/epoch\", step_metric=\"epoch\")\n",
    "wandb.define_metric(\"test_loss/epoch\", step_metric=\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d96ed02-d56d-4356-9a8c-028f685d0a83",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "with wandb logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "64979afc-5308-4652-bd7d-c7907bff625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|                                                              | 0/3 [00:00<?, ?it/s]\n",
      "Training Epoch 1/3:   0%|                                                 | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch 1/3:   8%|███▏                                     | 1/13 [00:00<00:01,  9.14it/s]\u001b[A\n",
      "Training Epoch 1/3:  15%|██████▎                                  | 2/13 [00:00<00:01,  9.58it/s]\u001b[A\n",
      "Training Epoch 1/3:  31%|████████████▌                            | 4/13 [00:00<00:00, 10.06it/s]\u001b[A\n",
      "Training Epoch 1/3:  38%|███████████████▊                         | 5/13 [00:00<00:01,  4.83it/s]\u001b[A\n",
      "Training Epoch 1/3:  54%|██████████████████████                   | 7/13 [00:01<00:00,  6.47it/s]\u001b[A\n",
      "Training Epoch 1/3:  69%|████████████████████████████▍            | 9/13 [00:01<00:00,  7.63it/s]\u001b[A\n",
      "Training Epoch 1/3:  85%|█████████████████████████████████▊      | 11/13 [00:01<00:00,  8.46it/s]\u001b[A\n",
      "Training Epoch 1/3: 100%|████████████████████████████████████████| 13/13 [00:01<00:00,  9.49it/s]\u001b[A\n",
      "Epochs:  33%|██████████████████                                    | 1/3 [00:02<00:04,  2.07s/it]\u001b[A\n",
      "Training Epoch 2/3:   0%|                                                 | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch 2/3:  15%|██████▎                                  | 2/13 [00:00<00:01,  6.89it/s]\u001b[A\n",
      "Training Epoch 2/3:  31%|████████████▌                            | 4/13 [00:00<00:01,  8.70it/s]\u001b[A\n",
      "Training Epoch 2/3:  46%|██████████████████▉                      | 6/13 [00:00<00:00,  9.50it/s]\u001b[A\n",
      "Training Epoch 2/3:  62%|█████████████████████████▏               | 8/13 [00:01<00:00,  7.66it/s]\u001b[A\n",
      "Training Epoch 2/3:  77%|██████████████████████████████▊         | 10/13 [00:01<00:00,  8.50it/s]\u001b[A\n",
      "Training Epoch 2/3:  92%|████████████████████████████████████▉   | 12/13 [00:01<00:00,  6.90it/s]\u001b[A\n",
      "Epochs:  67%|████████████████████████████████████                  | 2/3 [00:04<00:02,  2.04s/it]\u001b[A\n",
      "Training Epoch 3/3:   0%|                                                 | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "Training Epoch 3/3:   8%|███▏                                     | 1/13 [00:00<00:02,  4.04it/s]\u001b[A\n",
      "Training Epoch 3/3:  23%|█████████▍                               | 3/13 [00:00<00:01,  7.44it/s]\u001b[A\n",
      "Training Epoch 3/3:  38%|███████████████▊                         | 5/13 [00:00<00:00,  8.77it/s]\u001b[A\n",
      "Training Epoch 3/3:  54%|██████████████████████                   | 7/13 [00:00<00:00,  9.46it/s]\u001b[A\n",
      "Training Epoch 3/3:  69%|████████████████████████████▍            | 9/13 [00:01<00:00,  8.32it/s]\u001b[A\n",
      "Training Epoch 3/3:  85%|█████████████████████████████████▊      | 11/13 [00:01<00:00,  7.29it/s]\u001b[A\n",
      "Training Epoch 3/3: 100%|████████████████████████████████████████| 13/13 [00:01<00:00,  8.53it/s]\u001b[A\n",
      "Epochs: 100%|██████████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Training finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "global_step = 0  # for wandb log\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in tqdm(range(config.epoch), desc=\"Epochs\"):\n",
    "    model.train()  # switch to train mode\n",
    "    train_loss_list = []\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}/{config.epoch}\", leave=False):\n",
    "        batch = batch[0]\n",
    "        clean_data = batch.to(config.device)\n",
    "\n",
    "        # sample noise to add to the sequences\n",
    "        noise = torch.randn_like(batch).to(config.device)\n",
    "        \n",
    "        # sample a random timestep for each sequence\n",
    "        timesteps = torch.randint(\n",
    "            0, scheduler.num_train_timesteps, (batch.size(0),), device=config.device\n",
    "        ).long()\n",
    "\n",
    "        # add noise to the clean sequences\n",
    "        noisy_seq = scheduler.add_noise(clean_data, noise, timesteps)\n",
    "\n",
    "        # predict the noise added by scheduler\n",
    "        noise_pred = model(noisy_seq, timesteps, return_dict=False)\n",
    "        loss = criterion(noise_pred, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # logs\n",
    "        train_loss_list.append(loss.mean().item())\n",
    "        global_step += 1\n",
    "        wandb.log({\"train_loss/batch\": loss.item(), \"lr\": lr_scheduler.get_last_lr()[0], \"global_step\": global_step})\n",
    "\n",
    "    # end of one epoch (all data has been used to train model once)\n",
    "    ## evalueation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss_list = []\n",
    "        for val_batch in val_dataloader:\n",
    "            val_batch = val_batch[0]\n",
    "            clean_data = val_batch.to(config.device)\n",
    "            val_noise = torch.randn_like(val_batch).to(config.device)\n",
    "            val_timesteps = torch.randint(\n",
    "                0, scheduler.num_train_timesteps, (val_batch.size(0),), device=config.device\n",
    "            ).long()\n",
    "            val_noisy_seq = scheduler.add_noise(clean_data, val_noise, val_timesteps)\n",
    "\n",
    "            val_noise_pred = model(val_noisy_seq, val_timesteps, return_dict=False)\n",
    "            val_loss = criterion(val_noise_pred, val_noise)\n",
    "\n",
    "            val_loss_list.append(val_loss.mean().item())\n",
    "\n",
    "        # log epoch results\n",
    "        train_loss = sum(train_loss_list) / len(train_loss_list)\n",
    "        val_loss = sum(val_loss_list) / len(val_loss_list)\n",
    "        wandb.log({\"train_loss/epoch\": train_loss, \"test_loss/epoch\": val_loss, \"epoch\": epoch})\n",
    "\n",
    "        # save the best model for now\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            TIME = str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "            torch.save(model.state_dict(), os.path.join(config.save_path, TIME+f\"_5utr-diffusion_best_unet_model.pt\"))\n",
    "    \n",
    "    # model log\n",
    "    if epoch % config.save_model_epochs == 0 and epoch != 0:\n",
    "        TIME = str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "        pt_file = os.path.join(config.save_path, TIME+f\"_5utr-diffusion_unet_epoch_{epoch}.pt\")\n",
    "        torch.save(model.state_dict(), pt_file)\n",
    "        \n",
    "TIME = str(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "torch.save(model.state_dict(), os.path.join(config.save_path, TIME+\"_final_unet_model.pt\"))\n",
    "print(\">>> Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a60d4b-850c-4972-bfbf-b69964456508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc8938-021e-460f-94f9-b9a895d3c8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48283d07-bfd2-49a7-8124-df0ad0eed90c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
